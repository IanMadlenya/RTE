{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "from tqdm import tqdm\n",
    "directory = \"./data as csv/2013/\" \n",
    "os.chdir(directory)\n",
    "#print(glob.glob(\"*csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_files = glob.glob(\"*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P4503501.201701071300 .csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broken_files_13 = ['P4503501.201301180800 .csv',\n",
    " 'P4503501.201301250900 .csv',\n",
    " 'P4503501.201302101100 .csv',\n",
    " 'P4503501.201301180400 .csv',\n",
    " 'P4503501.201302101000 .csv',\n",
    " 'P4503501.201301261200 .csv',\n",
    " 'P4503501.201302140700 .csv',\n",
    " 'P4503501.201303130900 .csv',\n",
    " 'P4503501.201301171200 .csv',\n",
    " 'P4503501.201301170800 .csv',\n",
    " 'P4503501.201302140900 .csv',\n",
    " 'P4503501.201302101300 .csv',\n",
    " 'P4503501.201302140500 .csv',\n",
    " 'P4503501.201301171000 .csv',\n",
    " 'P4503501.201301181300 .csv',\n",
    " 'P4503501.201301180300 .csv',\n",
    " 'P4503501.201301161200 .csv',\n",
    " 'P4503501.201302140800 .csv',\n",
    " 'P4503501.201302101200 .csv',\n",
    " 'P4503501.201301250800 .csv',\n",
    " 'P4503501.201301181000 .csv',\n",
    " 'P4503501.201302141100 .csv',\n",
    " 'P4503501.201302140400 .csv',\n",
    " 'P4503501.201301180600 .csv',\n",
    " 'P4503501.201301180700 .csv',\n",
    " 'P4503501.201301170700 .csv',\n",
    " 'P4503501.201301171400 .csv',\n",
    " 'P4503501.201303131000 .csv',\n",
    " 'P4503501.201301261100 .csv',\n",
    " 'P4503501.201302140300 .csv',\n",
    " 'P4503501.201301180200 .csv',\n",
    " 'P4503501.201301181400 .csv',\n",
    " 'P4503501.201302100900 .csv',\n",
    " 'P4503501.201301251000 .csv',\n",
    " 'P4503501.201301171100 .csv',\n",
    " 'P4503501.201301170900 .csv',\n",
    " 'P4503501.201302141200 .csv',\n",
    " 'P4503501.201301181100 .csv',\n",
    " 'P4503501.201301250600 .csv',\n",
    " 'P4503501.201301250700 .csv',\n",
    " 'P4503501.201301171300 .csv',\n",
    " 'P4503501.201302100800 .csv',\n",
    " 'P4503501.201301180900 .csv',\n",
    " 'P4503501.201301181200 .csv',\n",
    " 'P4503501.201301261000 .csv',\n",
    " 'P4503501.201301180500 .csv',\n",
    " 'P4503501.201301261300 .csv',\n",
    " 'P4503501.201302141000 .csv',\n",
    " 'P4503501.201302140600 .csv',\n",
    " 'P4503501.201301171500 .csv']\n",
    "broken_files_14 = ['P4503501.201412291100 .csv',\n",
    " 'P4503501.201412310800 .csv',\n",
    " 'P4503501.201412301200 .csv',\n",
    " 'P4503501.201412311000 .csv',\n",
    " 'P4503501.201412291000 .csv',\n",
    " 'P4503501.201412290900 .csv',\n",
    " 'P4503501.201412310700 .csv',\n",
    " 'P4503501.201412310400 .csv',\n",
    " 'P4503501.201412300800 .csv',\n",
    " 'P4503501.201412301300 .csv',\n",
    " 'P4503501.201412301000 .csv',\n",
    " 'P4503501.201412311400 .csv',\n",
    " 'P4503501.201412290800 .csv',\n",
    " 'P4503501.201412301400 .csv',\n",
    " 'P4503501.201412300900 .csv',\n",
    " 'P4503501.201412310600 .csv',\n",
    " 'P4503501.201412311300 .csv',\n",
    " 'P4503501.201412310500 .csv',\n",
    " 'P4503501.201412311100 .csv',\n",
    " 'P4503501.201412301100 .csv',\n",
    " 'P4503501.201412291200 .csv',\n",
    " 'P4503501.201412311200 .csv',\n",
    " 'P4503501.201412310900 .csv']\n",
    "broken_files_15 = []\n",
    "broken_files_16 = ['P4503501.201601181000 .csv',\n",
    " 'P4503501.201601181200 .csv',\n",
    " 'P4503501.201601180700 .csv',\n",
    " 'P4503501.201601180900 .csv',\n",
    " 'P4503501.201601181100 .csv',\n",
    " 'P4503501.201601180800 .csv']\n",
    "broken_files_17 = ['P4503501.201701071300 .csv',\n",
    " 'P4503501.201701070300 .csv',\n",
    " 'P4503501.201701070800 .csv',\n",
    " 'P4503501.201701151300 .csv',\n",
    " 'P4503501.201701070100 .csv',\n",
    " 'P4503501.201701070600 .csv',\n",
    " 'P4503501.201701220900 .csv',\n",
    " 'P4503501.201701211100 .csv',\n",
    " 'P4503501.201701160600 .csv',\n",
    " 'P4503501.201701210700 .csv',\n",
    " 'P4503501.201701161100 .csv',\n",
    " 'P4503501.201701170800 .csv',\n",
    " 'P4503501.201701211000 .csv',\n",
    " 'P4503501.201701180700 .csv',\n",
    " 'P4503501.201701161000 .csv',\n",
    " 'P4503501.201701201300 .csv',\n",
    " 'P4503501.201701190500 .csv',\n",
    " 'P4503501.201701261000 .csv',\n",
    " 'P4503501.201701170900 .csv',\n",
    " 'P4503501.201701210800 .csv',\n",
    " 'P4503501.201701170200 .csv',\n",
    " 'P4503501.201701220800 .csv',\n",
    " 'P4503501.201701180000 .csv',\n",
    " 'P4503501.201701070900 .csv',\n",
    " 'P4503501.201701181400 .csv',\n",
    " 'P4503501.201701171100 .csv',\n",
    " 'P4503501.201701061300 .csv',\n",
    " 'P4503501.201701200900 .csv',\n",
    " 'P4503501.201701211200 .csv',\n",
    " 'P4503501.201701221300 .csv',\n",
    " 'P4503501.201701180600 .csv',\n",
    " 'P4503501.201701070200 .csv',\n",
    " 'P4503501.201701271100 .csv',\n",
    " 'P4503501.201701151000 .csv',\n",
    " 'P4503501.201701070500 .csv',\n",
    " 'P4503501.201701170400 .csv',\n",
    " 'P4503501.201701200600 .csv',\n",
    " 'P4503501.201701170700 .csv',\n",
    " 'P4503501.201701231200 .csv',\n",
    " 'P4503501.201701180800 .csv',\n",
    " 'P4503501.201701151200 .csv',\n",
    " 'P4503501.201701061100 .csv',\n",
    " 'P4503501.201701151400 .csv',\n",
    " 'P4503501.201701171200 .csv',\n",
    " 'P4503501.201701061200 .csv',\n",
    " 'P4503501.201701231100 .csv',\n",
    " 'P4503501.201701171000 .csv',\n",
    " 'P4503501.201701190400 .csv',\n",
    " 'P4503501.201701150900 .csv',\n",
    " 'P4503501.201701190800 .csv',\n",
    " 'P4503501.201701170300 .csv',\n",
    " 'P4503501.201701181100 .csv',\n",
    " 'P4503501.201701201100 .csv',\n",
    " 'P4503501.201701221200 .csv',\n",
    " 'P4503501.201701180400 .csv',\n",
    " 'P4503501.201701231000 .csv',\n",
    " 'P4503501.201701071200 .csv',\n",
    " 'P4503501.201701181300 .csv',\n",
    " 'P4503501.201701070700 .csv',\n",
    " 'P4503501.201701180100 .csv',\n",
    " 'P4503501.201701181000 .csv',\n",
    " 'P4503501.201701160800 .csv',\n",
    " 'P4503501.201701190700 .csv',\n",
    " 'P4503501.201701170600 .csv',\n",
    " 'P4503501.201701071000 .csv',\n",
    " 'P4503501.201701261100 .csv',\n",
    " 'P4503501.201701260800 .csv',\n",
    " 'P4503501.201701180200 .csv',\n",
    " 'P4503501.201701260700 .csv',\n",
    " 'P4503501.201701260500 .csv',\n",
    " 'P4503501.201701160900 .csv',\n",
    " 'P4503501.201701180900 .csv',\n",
    " 'P4503501.201701171400 .csv',\n",
    " 'P4503501.201701180500 .csv',\n",
    " 'P4503501.201701181200 .csv',\n",
    " 'P4503501.201701230900 .csv',\n",
    " 'P4503501.201701261200 .csv',\n",
    " 'P4503501.201701160700 .csv',\n",
    " 'P4503501.201701170100 .csv',\n",
    " 'P4503501.201701161300 .csv',\n",
    " 'P4503501.201701260400 .csv',\n",
    " 'P4503501.201701070400 .csv',\n",
    " 'P4503501.201701170500 .csv',\n",
    " 'P4503501.201701161400 .csv',\n",
    " 'P4503501.201701180300 .csv',\n",
    " 'P4503501.201701201200 .csv',\n",
    " 'P4503501.201701171300 .csv',\n",
    " 'P4503501.201701201000 .csv',\n",
    " 'P4503501.201701071100 .csv',\n",
    " 'P4503501.201701260600 .csv',\n",
    " 'P4503501.201701161200 .csv',\n",
    " 'P4503501.201701200700 .csv',\n",
    " 'P4503501.201701261300 .csv',\n",
    " 'P4503501.201701170000 .csv',\n",
    " 'P4503501.201701231300 .csv',\n",
    " 'P4503501.201701260900 .csv',\n",
    " 'P4503501.201701210900 .csv',\n",
    " 'P4503501.201701221000 .csv',\n",
    " 'P4503501.201701151100 .csv',\n",
    " 'P4503501.201701190600 .csv',\n",
    " 'P4503501.201701221100 .csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3096/3096 [04:04<00:00, 12.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(all_files):\n",
    "    try:\n",
    "        data = pd.read_csv(file, usecols = ['station', 'date.validite', 'temperature', 'nebulosite'])\n",
    "        data.columns = ['station','Time', 'temperature', 'cloudiness']\n",
    "\n",
    "        data.index = data['Time'] \n",
    "\n",
    "        weather_data = pd.DataFrame()\n",
    "        temp = pd.DataFrame()\n",
    "\n",
    "        if file in broken_files_13:\n",
    "            # Some files have broken naming\n",
    "            stations = ['002', '005', '015', '027', '070', '110', '120', '130', '145', '149', '156','168', \n",
    "                        '180', '190', '222', '240', '255', '260', '280', '299', '434', '460', '481', '497', \n",
    "                        '510', '579', '588', '621', '630', '643', '645', '650', '675', '690', '747']\n",
    "            #print(file)\n",
    "\n",
    "            for station in stations: \n",
    "                temp = data.loc[data['station'] == station]\n",
    "                temperature_index = 'temperature-' + str(station)\n",
    "                cloudiness_index = 'cloudiness-' + str(station)\n",
    "                weather_data[temperature_index] = temp['temperature']\n",
    "                weather_data[cloudiness_index] = temp['cloudiness']\n",
    "\n",
    "        else:\n",
    "            stations = [2, 5, 15, 27, 70, 110, 120, 130, 145, 149, 156,\n",
    "                    168, 180, 190, 222, 240, 255, 260, 280, 299, 434,\n",
    "                    460, 481, 497, 510, 579, 588, 621, 630, 643, 645,\n",
    "                    650, 675, 690, 747]\n",
    "\n",
    "            for station in stations:\n",
    "                if station in stations[0:2]:\n",
    "                    temp = data.loc[data['station'] == station]\n",
    "                    temperature_index = 'temperature-00' + str(station)\n",
    "                    cloudiness_index = 'cloudiness-00' + str(station)\n",
    "                    weather_data[temperature_index] = temp['temperature']\n",
    "                    weather_data[cloudiness_index] = temp['cloudiness']\n",
    "\n",
    "                elif station in stations[2:5]:\n",
    "                    temp = data.loc[data['station'] == station]\n",
    "                    temperature_index = 'temperature-0' + str(station)\n",
    "                    cloudiness_index = 'cloudiness-0' + str(station)\n",
    "                    weather_data[temperature_index] = temp['temperature']\n",
    "                    weather_data[cloudiness_index] = temp['cloudiness']\n",
    "                else:\n",
    "                    temp = data.loc[data['station'] == station]\n",
    "                    temperature_index = 'temperature-' + str(station)\n",
    "                    cloudiness_index = 'cloudiness-' + str(station)\n",
    "                    weather_data[temperature_index] = temp['temperature']\n",
    "                    weather_data[cloudiness_index] = temp['cloudiness']\n",
    "\n",
    "        current_weather = pd.DataFrame(columns = weather_data.columns)\n",
    "        #current_weather = weather_data.loc[weather_data.index[5]:weather_data.index[6]]\n",
    "        current_weather = weather_data.loc[:]\n",
    "        #current_weather.drop(current_weather.index[1], inplace = True)\n",
    "        if current_weather.empty:\n",
    "            print('DataFrame is empty!')\n",
    "        current_weather.to_csv(file)\n",
    "        #print(file)\n",
    "    except:\n",
    "        print('Error with file: ' +  file)\n",
    "        error_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
